- Detection & Direction (Phase 2): The system can detect a person using YOLOv8 and track their horizontal movement direction. It maintains a person_detected state and updates a direction (LEFT_TO_RIGHT or RIGHT_TO_LEFT) once sufficient movement is observed. However, detection covers the entire frame – there is no concept of a restricted region. Any person anywhere in the frame triggers a detection event and direction tracking. Also, while direction is tracked, it’s not yet mapped to entry or exit semantics (the system logs “left_to_right” vs “right_to_left” but doesn’t know which means entry or exit).

- Dashboard & UI: Phase 2 provides a basic web dashboard (HTML page) showing the live camera feed (via an MJPEG stream) and detection status. Users can toggle detection on/off and see recent detection events (with timestamps and raw direction data). No UI exists for ROI configuration – the dashboard does not allow drawing a region or setting direction labels. The video feed displays the full camera view with no overlays or selectable regions.

- Database & Events: A SQLite database logs detection events and system logs (e.g., detection start, detection end with direction) via the DatabaseManager. For example, when a person is detected or lost, events like "detection_start" and "detection_end" (with the last direction) are saved. The DashboardManager keeps counts of total detections and direction counts (left-to-right vs right-to-left) in memory. Persistent settings for ROI or camera configuration are not stored – all detection parameters (e.g., confidence_threshold, frame size) come from a static config file, and no dynamic region settings exist yet.

- System Behavior: In the current state, whenever any person enters the camera frame, the system increments the global detection count and logs the direction of travel. Because there is no ROI filtering, footfall counts may include irrelevant movement (e.g., someone walking in view but not through the doorway). Also, without entry/exit mapping, the system doesn’t differentiate whether a count was an entry or an exit. These gaps will be addressed in Phase 3.

- Multi-Camera Considerations: At present, the application effectively handles a single camera (one CameraManager thread and one DetectionManager thread using a single device_id). The architecture is modular (managers could be instantiated per camera), but there is no UI or config support for multiple cameras yet. Phase 3 will ensure that the ROI and direction mapping is stored per camera (indexed by camera ID) so that adding another camera in the future would simply involve assigning a new ROI and direction configuration for that camera.