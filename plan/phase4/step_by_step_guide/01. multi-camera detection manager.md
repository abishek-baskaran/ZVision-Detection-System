# Multi-Camera Detection Manager – Phase 4

With multiple camera feeds, the detection system must run a separate person detection pipeline for each camera. We will refactor the `DetectionManager` to manage per-camera detection threads and maintain independent state (person present, direction, etc.) for each camera. The goal is to process each camera's frames in parallel while optimizing resource usage across all cameras.

---

## Refactoring DetectionManager for Multiple Cameras

Instead of a single detection loop tied to one `CameraManager`, the `DetectionManager` will orchestrate multiple loops — one for each camera in the `CameraRegistry`.

### Key Changes in DetectionManager:

- **Hold CameraRegistry**: Store a reference to the `CameraRegistry` (e.g., `self.camera_registry`) instead of a single `camera_manager`.
- **Multiple Threads**: Maintain a dictionary of detection threads (e.g., `self.detection_threads = {camera_id: thread}`) and a separate set of detection state variables for each camera.
- **Initialization**: When the `DetectionManager` is initialized (after cameras are set up), prepare detection structures for each available camera. This could include pre-loading the ML model for each thread or sharing a model if feasible.
- **Start/Stop**: Provide methods to start detection on all cameras (`start_all()`) or a specific camera (`start_camera(camera_id)`), and similarly stop detection.

### Example Modifications in `managers/detection_manager.py`

```python
class DetectionManager:
    def __init__(self, resource_provider, camera_registry, dashboard_manager=None, db_manager=None):
        self.camera_registry = camera_registry
        self.dashboard_manager = dashboard_manager
        self.db_manager = db_manager
        self.logger = resource_provider.get_logger()
        self.config = resource_provider.get_config()

        detection_config = self.config.get('detection', {})
        self.model_path = detection_config.get('model_path', 'yolov8n.pt')
        self.confidence_threshold = detection_config.get('confidence_threshold', 0.25)
        self.person_class_id = detection_config.get('person_class_id', 0)

        self.states = {}
        self.detection_threads = {}
        self.model = YOLO(self.model_path)

        self.logger.info("Multi-camera DetectionManager initialized")
```

---

## Per-Camera Detection Loop

Implement a detection loop function that runs for each camera. This could be a method inside `DetectionManager`:

```python
def _run_detection_for_camera(self, camera_id):
    state = {"person_detected": False, "no_person_counter": 0, "current_direction": self.DIRECTION_UNKNOWN}
    self.states[camera_id] = state
    camera = self.camera_registry.get_camera(camera_id)
    if not camera:
        return

    model = YOLO(self.model_path) if not self.model else self.model
    idle_interval = 1.0 / self.idle_fps
    active_interval = 1.0 / self.active_fps
    last_frame_time = 0

    while camera.is_running:
        frame = camera.get_latest_frame()
        if frame is None:
            time.sleep(0.1)
            continue

        interval = active_interval if state["person_detected"] else idle_interval
        if time.time() - last_frame_time < interval:
            time.sleep(0.01)
            continue
        last_frame_time = time.time()

        results = model(frame, conf=self.confidence_threshold, verbose=False)
        person_found = False
        bbox_center_x = None

        for r in results:
            for box in r.boxes:
                if int(box.cls[0]) == self.person_class_id:
                    if camera_id in self.roi_settings:
                        cx = (float(box.xyxy[0][0]) + float(box.xyxy[0][2])) / 2
                        cy = (float(box.xyxy[0][1]) + float(box.xyxy[0][3])) / 2
                        rx1, ry1, rx2, ry2 = self.roi_settings[camera_id]["coords"]
                        if not (rx1 <= cx <= rx2 and ry1 <= cy <= ry2):
                            continue
                    person_found = True
                    bbox_center_x = (float(box.xyxy[0][0]) + float(box.xyxy[0][2])) / 2
                    break
            if person_found:
                break

        self._update_detection_state(camera_id, person_found, bbox_center_x)
```

---

## Detection State Update

```python
def _update_detection_state(self, camera_id, person_present, center_x):
    state = self.states[camera_id]
    if person_present:
        if not state["person_detected"]:
            state["person_detected"] = True
            state["no_person_counter"] = 0
            state["current_direction"] = self.DIRECTION_UNKNOWN
            if self.dashboard_manager:
                self.dashboard_manager.record_detection(camera_id)
            if self.db_manager:
                self.db_manager.log_detection_event("detection_start", camera_id=camera_id)
            if self.api_manager:
                self.api_manager.emit_event("detection_start", {"camera": camera_id})

        if center_x is not None:
            self._record_position(camera_id, center_x)
    else:
        if state["person_detected"]:
            state["no_person_counter"] += 1
            if state["no_person_counter"] >= 5:
                state["person_detected"] = False
                dir_str = self._get_direction_string(camera_id)
                event_type = "detection_end"
                if camera_id in self.roi_settings:
                    entry_dir = self.roi_settings[camera_id]["entry_direction"]
                    if dir_str == "left_to_right":
                        event_type = "entry" if entry_dir == "LTR" else "exit"
                    elif dir_str == "right_to_left":
                        event_type = "entry" if entry_dir == "RTL" else "exit"

                if self.db_manager:
                    self.db_manager.log_detection_event(event_type, direction=dir_str, camera_id=camera_id)
                if self.dashboard_manager:
                    self.dashboard_manager.record_footfall(event_type, camera_id=camera_id)
                if self.api_manager:
                    self.api_manager.emit_event(event_type, {
                        "camera": camera_id,
                        "event": event_type,
                        "direction": dir_str
                    })
```

---

## Direction Tracking

Track center X positions per camera to determine direction:

```python
self.position_history[camera_id] = deque(maxlen=20)
```

---

## Resource Balancing & Prioritization

To avoid CPU overload:

- **Adaptive FPS**: Lower FPS for lower priority cameras during high CPU usage.
- **Priorities**: Assign importance to cameras; high-priority ones get more resources.
- **Thread Control**: Use staggered starts or thread pools in future to prevent spikes.

---

## Unified vs Per-Camera Aggregation

- **Per-Camera**: Use `get_detection_status(camera_id)` to return specific camera status.
- **Global View**: `/api/status` can return list of camera statuses or summary.
- **Dashboard**: Track per-camera counts and state in DashboardManager.

---

## \sequentialthinking Subtasks Outline:

1. **Update DetectionManager Init**: Change the constructor to accept `camera_registry` and remove the single `camera_manager` reference. Initialize structures (`threads` dict, `state` dict, `position_history` dict) for multiple cameras.

2. **Implement Per-Camera Thread Function**: Write a `_run_detection_for_camera(camera_id)` method that encapsulates detection for one camera, adapting YOLO inference, ROI check, and direction tracking.

3. **Manage Threads Start/Stop**: Implement `start_all()` to spawn a thread for each camera. Implement `start_camera(id)` for individual camera detection. Include stopping logic for shutdown or camera removal.

4. **Incorporate ROI & Direction Config**: Use stored ROI and `entry_direction` for each camera (from DB or config). Store in `self.roi_settings[camera_id]`.

5. **Logging and Events**: Modify `DatabaseManager.log_detection_event` and `DashboardManager.record_footfall` to include `camera_id`. Ensure all events are tagged correctly.

6. **Test Multi-Camera Detection**: Run system with at least two cameras. Verify both detect independently, generate separate events, and can be stopped individually. Check event logs for correct `camera_id`.

\endsequentialthinking
