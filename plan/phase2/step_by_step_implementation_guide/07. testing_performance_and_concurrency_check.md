Finally, thoroughly test the integrated system and make adjustments for concurrency or performance issues:
- Concurrent Operation: Verify that the camera stream and detection thread truly run in parallel:
    - When a person is in view, the detection thread will ramp up to ~5 FPS processing. The camera should still deliver frames for the video stream at a smooth rate. Watch the stream – if it freezes or significantly lags while detection is busy, there might be a GIL contention issue. In such case, consider lowering active_fps a bit (to reduce CPU strain) or using the multiprocessing approach (run detection in a separate process to utilize another core). On a Pi 5, with its multiple cores, you could pin detection on a separate core via multiprocessing if needed.
    - Check that Flask’s request handling (for the video_feed and API calls) remains responsive even under detection load. Flask by default handles each request in a separate thread; since our heavy tasks (YOLO inference) occur in a background thread and OpenCV is C-optimized, the GIL should be released during frame capture and model inference. This means the web server can still serve images and respond to toggling.
- Resource Usage: Monitor CPU and memory on the Pi:
    - The YOLOv8 model load and inference will use a chunk of CPU (and potentially some RAM for the model). Streaming JPEGs also uses CPU. On Pi 5, this is likely manageable, but if CPU usage is near 100% with both running, you might take measures: e.g., reduce camera resolution or frame rate (in config.yaml), use a smaller model or lower confidence threshold (to skip some detections), etc.
    - If using WebSocket with eventlet, ensure it’s using monkey-patching or that it doesn’t conflict with the camera thread. Flask-SocketIO with default settings should be fine, but test under load.
- Functional Testing: Simulate scenarios:
    - Idle mode: Start the system with no person in front. The dashboard should show “Person Detected: No” and video streaming. CPU usage should be low (1 FPS detection). Metrics should not increase.
    - Person enters frame: When you appear on camera, within ~1 second the system should detect you. The dashboard “Person Detected” should switch to Yes (green indicator) and “Movement Direction” may initially say “unknown” until you move sufficiently. The footfall count increments by 1 on first detection. The detection thread now processes ~5 fps – verify the video still updates (it might drop to a slightly lower FPS but should still be reasonably smooth).
    - Move across frame: Walk left-to-right or right-to-left. The system should log the direction. The UI should update “Movement Direction” to left_to_right or right_to_left, and the direction count in metrics should increment appropriately. Check that the direction event is captured only once per crossing (the code uses a threshold and logs when direction changes).
    - Person leaves frame: After you disappear, within a second or two the system should mark “Person Detected: No” again and switch back to idle mode (log message “switching to idle mode” in backend). The footfall count remains the same (it doesn’t increment on disappearance, only on appearance). If you stay out for more than a few seconds, the detection thread should be effectively sleeping most of each second (1 FPS).
    - Toggle detection off: While no one is in frame, press “Pause Detection”. The status should indicate detection is off. Confirm that the detection thread stops (you can check logs or CPU drop). The video stream should continue unaffected. If you walk in front of the camera now, the “Person Detected” remains No (since detection isn’t running) – effectively the system ignores you. This simulates the system idling with camera on but no processing.
    - Toggle detection on: Press “Resume Detection”. The detection thread should start up and quickly detect the person if still in frame. The footfall count should increment now (assuming it considered it a new detection event after the pause). Ensure no crashes or weird behavior when toggling.
    - Rapid toggle (if needed): Try toggling on/off quickly a few times to ensure stability (though in real use this is rare).
- Database Verification: Query the SQLite zvision.db after some testing to ensure events are logged: you should see entries in detection_events for each detection start (event_type = 'detection_start'), each detection end, and direction events. This will confirm that backend logging matches the dashboard display.
- Cleanup and Deployment: Make sure the log files are rotating as per config (so the Pi’s disk doesn’t fill with logs). The system can be started on boot if needed (consider a systemd service for python main.py). Because everything is in one process (unless multiprocessing is added), deployment is straightforward.
