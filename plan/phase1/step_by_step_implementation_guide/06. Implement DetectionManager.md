- Create managers/detection_manager.py and implement DetectionManager:
    - Load the YOLOv8 model in the constructor. If using Ultralytics, ensure the model file (e.g., yolov8n.pt) is present or handle downloading. Loading might take a bit of time on the Pi, so log that the model is loading to know the status.
    - Implement the _detection_loop logic carefully:
        - Use the timing mechanism to decide when to process the next frame. The pseudocode uses time.time() comparisons to regulate processing frequency. Alternatively, you could simply sleep(idle_interval) at the end of each loop when idle (and a shorter sleep when active), but using a next_frame_time allows more precise control if needed.
        - Retrieve a frame from the camera. Here, using self.camera.frame_queue.get() will block until a frame is available. This is fine because the camera is running continuously. However, consider adding a small timeout to get() so that the loop can check _stop_event periodically even if no frames are arriving. For example: frame = self.camera.frame_queue.get(timeout=0.5) inside a try/except, so that if the camera stops providing frames, the detection loop can still break out gracefully.
        - Run the model on the frame. If using Ultralytics YOLO, the code shown will yield results. Ensure to run inference in a way that is efficient on CPU (the Ultralytics library by default might use PyTorch and could utilize any hardware acceleration if available). If performance is an issue, consider using OpenCV’s DNN module with a YOLOv8 ONNX model, or a smaller model. But initially, test with the chosen approach.
        - Process the results to find persons. You’ll need to know the class ID for "person" in the YOLO model's labels (commonly 0 for COCO). The pseudocode assumed index 5 in the detection array is class and index 4 is confidence – adapt this to actual library output. In Ultralytics, you might do:
```
for r in results:
    for box in r.boxes:
        cls = int(box.cls[0])
        conf = float(box.conf[0])
        if cls == 0 and conf >= self.conf_threshold:
            # person detected
```
Adjust as necessary.
        - Implement state transitions: If a person appears, log and switch to active mode; if disappears, switch to idle. Use the logic described to avoid flicker (require a few consecutive frames to confirm disappearance).
        - Implement direction calculation: track centroids in last_positions. You might refine this by using more frames or adding a simple moving average. The threshold of movement to declare direction might need tuning depending on camera view (5 pixels may be too low or high).
        - Call the DashboardManager’s methods at appropriate times (record_detection when a new person is detected, record_direction when you’ve determined the direction). Also call DatabaseManager’s log_event to record these events persistently.
        -- Use logging liberally in this loop at least during development, because it’s the most complex part. Log when detections happen and what the outcomes are (e.g., "Detected person, center=(100,50)", "Moving direction: right", "No person for 5 frames, returning to idle").
    -- Test the DetectionManager in isolation as much as possible: this is tricky without a full camera feed, but you could simulate by reading frames from a video file or images. One approach is to feed a static image with a person to the model to see if detection works, then another image without a person to test transitions. However, the best test is integration with CameraManager: start CameraManager and DetectionManager together and see if it correctly identifies when a person is in frame. On the Pi, monitor CPU usage to ensure 5 FPS active processing is sustainable. If it’s too high, consider using an even smaller model or reducing frame size.