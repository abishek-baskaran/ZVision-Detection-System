**Purpose:** The DetectionManager handles running the person detection model (YOLOv8) on frames and tracking the motion of detected persons. It adjusts its processing rate based on whether a person is present or not to balance responsiveness with CPU usage. It is the core of the intelligent behavior: it identifies if a person is in the frame and determines their direction of movement. 

### Responsibilities:
- Load the YOLOv8 model (optimized for the Pi, possibly a smaller variant like YOLOv8n). This can be done via a library (e.g., Ultralytics YOLO).
- Continuously receive frames from the CameraManager (via the queue or a direct call) in a background thread.
- Perform person detection on frames. When no person is detected for a while, switch to idle mode (process infrequently). When a person is detected, switch to active mode (process many frames to track movement).
- Determine the direction the person is moving. This can be done by tracking the person’s bounding box between frames:
    - A simple approach: record the centroid of the person’s bounding box for each frame and observe the change. If the x-coordinate is increasing over time, the person is moving right; if decreasing, moving left. (You could also track the y-coordinate if vertical movement matters, but assume horizontal direction for walking direction.)
    - Optionally, use a tracking algorithm: e.g., a Kalman filter + Hungarian matching (like Deep SORT) for multi-object or more robust tracking, or optical flow to track movement vectors. For our design, a basic centroid tracking will be described for simplicity.
- Raise events or update state when key detection events happen (e.g., person first detected, person disappeared, direction determined). This information will be passed to DashboardManager or logged via DatabaseManager for analytics.
- Ensure thread-safe communication of detection results (e.g., a shared state person_present flag or last seen direction) for API access.

### Pseudocode Skeleton:

```
# managers/detection_manager.py
import threading
import time
# assume YOLO from ultralytics is installed for this pseudocode
from ultralytics import YOLO

class DetectionManager:
    def __init__(self, resource_provider, camera_manager, dashboard_manager, db_manager):
        self.config = resource_provider
        self.camera = camera_manager
        self.dashboard = dashboard_manager
        self.db = db_manager
        self.logger = resource_provider.get_logger("DetectionManager")
        # Load YOLOv8 model (e.g., nano version for speed)
        model_path = self.config.get_config("yolo_model_path", "yolov8n.pt")
        self.model = YOLO(model_path)  # this loads the model into memory
        # Detection parameters
        self.idle_interval = self.config.get_config("idle_interval", 1.0)   # seconds between frames when idle
        self.active_interval = self.config.get_config("active_interval", 0.2)  # ~5 FPS when active
        self.conf_threshold = self.config.get_config("confidence_threshold", 0.5)  # YOLO confidence threshold for person
        # State variables
        self.person_present = False
        self.last_detection_time = 0
        self.last_positions = []  # list of recent centroid positions for tracking
        self.current_direction = None  # "left" or "right" or None
        self._stop_event = threading.Event()

    def start(self):
        self.logger.info("Starting DetectionManager thread...")
        self._stop_event.clear()
        threading.Thread(target=self._detection_loop, daemon=True).start()

    def _detection_loop(self):
        """Continuously process frames for detection."""
        next_frame_time = time.time()
        no_person_counter = 0  # count consecutive frames with no person
        while not self._stop_event.is_set():
            # Throttle loop to idle or active interval
            now = time.time()
            if now < next_frame_time:
                time.sleep(min(next_frame_time - now, 0.01))
                continue  # skip until next scheduled frame time
            next_frame_time = now + (self.active_interval if self.person_present else self.idle_interval)
            # Get the latest frame from CameraManager
            frame = self.camera.frame_queue.get()  # block until a frame is available
            # (If using get_frame() method instead: frame = self.camera.get_frame(); if frame is None: continue)
            if frame is None:
                continue
            # Run YOLOv8 inference on the frame
            results = self.model(frame, verbose=False)[0]  # get first result
            # Filter detections for person class (assuming class 'person' has specific ID, e.g., 0)
            persons = [det for det in results.boxes.data.tolist() if int(det[5]) == 0 and det[4] >= self.conf_threshold]
            if persons:
                # Person detected
                no_person_counter = 0
                if not self.person_present:
                    # Person just appeared (transition from no person to person present)
                    self.person_present = True
                    self.last_positions.clear()
                    self.current_direction = None
                    self.logger.info("Person detected – switching to active mode")
                    # Log event to database and dashboard
                    self.db.log_event("person_appeared", {"time": now})
                    self.dashboard.record_detection()  # increment detection count
                # Track position (take the first person's bounding box center for simplicity)
                x1, y1, x2, y2 = persons[0][0:4]  # assuming det format: [x1,y1,x2,y2,conf,class_id]
                cx = (x1 + x2) / 2
                cy = (y1 + y2) / 2
                self.last_positions.append((cx, cy))
                # Keep only recent few positions to smooth noise
                if len(self.last_positions) > 5:
                    self.last_positions.pop(0)
                # Determine direction if enough points
                if len(self.last_positions) >= 2:
                    if cx - self.last_positions[0][0] > 5:   # moved right (threshold in pixels)
                        self.current_direction = "right"
                    elif cx - self.last_positions[0][0] < -5:  # moved left
                        self.current_direction = "left"
                    # (If needed, also consider vertical movement or a more robust average of movement)
                # If a direction is determined, we can log it (only log the first time direction becomes known)
                # (Or continuously update a direction metric in dashboard)
                if self.current_direction and len(self.last_positions) == 2:
                    self.logger.info(f"Direction determined: {self.current_direction}")
                    self.db.log_event("direction", {"direction": self.current_direction, "time": now})
                    self.dashboard.record_direction(self.current_direction)
            else:
                # No person detected in this frame
                if self.person_present:
                    no_person_counter += 1
                    # If person was present but now missing for a few frames, consider they have disappeared
                    if no_person_counter >= 5:  # e.g., 5 consecutive misses
                        self.person_present = False
                        self.current_direction = None
                        self.last_positions.clear()
                        self.logger.info("Person lost – switching to idle mode")
                        self.db.log_event("person_disappeared", {"time": now})
                # else, if already in no-person state, just remain idle
            # Loop continues...
        self.logger.info("DetectionManager stopped.")

    def stop(self):
        """Stop the detection loop."""
        self._stop_event.set()

    def get_status(self):
        """Return current detection status for API (thread-safe retrieval of key info)."""
        return {
            "person_present": self.person_present,
            "current_direction": self.current_direction
        }
```

**Explanation:** The DetectionManager runs _detection_loop in a thread. It uses a timing mechanism (next_frame_time) to control processing frequency. In idle mode (no person), it processes at most one frame per second (idle_interval). In active mode (person present), it processes at ~5 FPS (active_interval = 0.2s). We fetch frames from the CameraManager’s frame_queue. Using get() without timeout will block until a frame is available, effectively syncing with the camera feed. 

We run the YOLO model on the frame. Here we assume using the Ultralytics YOLO API (the code uses results = self.model(frame) which returns predictions; we then filter for class "person" and confidence >= threshold). In practice, you would adjust this code to match the YOLOv8 library’s output format (the pseudocode assumes results.boxes.data.tolist() yields [x1,y1,x2,y2,confidence,class] for each detection). 

If a person is detected:
- If this is a new appearance (previously no person), we mark person_present = True, clear any old positions, and log an event that a person appeared. We notify DashboardManager (e.g., record_detection() could increment a counter of total detections).
- We compute the centroid (cx, cy) of the detected person’s bounding box and append it to last_positions. We limit this list to recent points (e.g., last 5) to avoid indefinite growth.
- If we have at least two points, we compare the latest position to the oldest in the list to infer direction. For example, if the x-coordinate moved significantly to the right (cx increased), we set current_direction = "right". If it decreased, current_direction = "left". (We use a small threshold like 5 pixels to avoid noise; in a real system you might use a more robust method or average multiple differences.)
- The first time we ascertain a direction (we check if current_direction was just set), we log it via DatabaseManager and inform DashboardManager (record_direction might log it or update a stat like “last direction” or count how many left/right moves).
- If multiple people were present, this simple approach only tracks the first detected person. In a more advanced implementation, you might track multiple by IDs (e.g., integrating Deep SORT to maintain identities and directions for each). The design leaves room to plug in such an algorithm if needed, but on resource-constrained Pi, handling one person at a time might be the initial target.

If no person is detected in the frame:
- If we were in active mode (person was present) and now get consecutive frames with no detections, we increment a counter. After a few such frames (e.g., 5 frames * 0.2s = 1s with no person), we assume the person has left the scene. Then we mark person_present = False, clear direction/positions, and log a “person disappeared” event. The DetectionManager then falls back to idle mode (processing every 1s as controlled by idle_interval).
- If we were already idle (no person for a long time), we simply remain in that state and continue checking one frame per second.

The get_status() method provides a snapshot of the current state (whether a person is present and what the current determined direction is). This can be used by the APIManager to report status without needing to directly access internal variables (and it can include any other relevant info easily). 

Throughout, we use the logger to record important milestones (person detected/lost, direction changes) for debugging and audit. The DatabaseManager (db.log_event) is used to persist events, which could later be retrieved for analysis (for example, counting how many times someone walked left vs right in a day). These log_event calls imply corresponding methods in DatabaseManager we will define.